{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rec_gym\n",
    "from rec_gym.wrappers import StatsWrapper, FlattenObservationsWrapper\n",
    "from copy import deepcopy\n",
    "from rec_gym.runner import run_experiment\n",
    "\n",
    "from agents.baselines import RandomAgent, DotProdAgent, MFAgent\n",
    "from agents.dqn import Qagent\n",
    "from agents.ddpg import DDPGAgent\n",
    "import tensorflow as tf\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from agents import PopularityAgent, SVDAgent, LinUCB, HLinUCB, create_drr_agent, RandomAgent\n",
    "\n",
    "from rec_gym.wrappers import StatsWrapper, DRR_EmbBaselinesWrapper, ExplicitUserItemWrapper, DRR_ExplicitUserItemWrapper, DRR_BaselinesWrapper\n",
    "\n",
    "from rec_gym.envs import MovieLensDRR, MovieLens100\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(agent, env, train_steps, eval_steps):\n",
    "\n",
    "    done = True\n",
    "    r = None\n",
    "    info = None\n",
    "\n",
    "    ts = []\n",
    "    for _ in tqdm.tqdm_notebook(range(train_steps)):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        if done:\n",
    "            if r:\n",
    "                agent.end_episode(r, info)\n",
    "            obs = env.reset()\n",
    "            a = agent.begin_episode(obs)\n",
    "    #         print(a)\n",
    "            obs, r, done, info = env.step(a)\n",
    "    #         done = False\n",
    "        else:\n",
    "            a = agent.step(r, obs, info)\n",
    "    #         print(a)\n",
    "            obs, r, done, info = env.step(a)\n",
    "        \n",
    "        end = time.time()\n",
    "        ts.append(end-start)\n",
    "    \n",
    "    mean_train_t = np.mean(ts)\n",
    "    \n",
    "    done = True\n",
    "    r = None\n",
    "    info = None\n",
    "    agent.eval_mode = True\n",
    "    \n",
    "    ts = []\n",
    "    for _ in tqdm.tqdm_notebook(range(eval_steps)):\n",
    "        start = time.time()\n",
    "        if done:\n",
    "            if r:\n",
    "                agent.end_episode(r, info)\n",
    "            obs = env.reset()\n",
    "            a = agent.begin_episode(obs)\n",
    "            obs, r, done, info = env.step(a)\n",
    "        else:\n",
    "            a = agent.step(r, obs, info)\n",
    "            obs, r, done, info = env.step(a)\n",
    "        end = time.time()\n",
    "        ts.append(end-start)\n",
    "    \n",
    "    mean_eval_t = np.mean(ts)\n",
    "    \n",
    "    return mean_train_t, mean_eval_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_random_agent_and_env(base_env):\n",
    "    env = deepcopy(base_env)\n",
    "    wrapped = FlattenObservationsWrapper(env)\n",
    "    action_size = env.unwrapped.n_rec\n",
    "    agent = RandomAgent(action_size = action_size)    \n",
    "    return agent, wrapped, env\n",
    "\n",
    "def prepare_ddpg_agent_and_env(base_env):\n",
    "    env = deepcopy(base_env)\n",
    "    wrapped = DRR_BaselinesWrapper(env)\n",
    "    unwrapped = env.unwrapped\n",
    "\n",
    "    state_dim = unwrapped.embedding_dimension\n",
    "    action_dim = unwrapped.embedding_dimension\n",
    "    action_size = unwrapped.n_rec\n",
    "\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.InteractiveSession()\n",
    "    agent = DDPGAgent(action_size = action_size,\n",
    "                     state_dim = state_dim,\n",
    "                     action_dim = action_dim,\n",
    "                     gamma = .9,\n",
    "                     sess = sess,\n",
    "                     optimizer = tf.train.AdamOptimizer(\n",
    "                         learning_rate=0.01\n",
    "                     ),\n",
    "                     max_tf_checkpoints_to_keep = 3,\n",
    "                     experience_size = 300,\n",
    "                     per = True,\n",
    "                     batch_size = 64, \n",
    "                     start_steps = 400)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return agent, wrapped, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(session_size=10, seed=123):\n",
    "#     env = MovieLensDRR(embedding_dimension=20, \n",
    "#              n_items_to_recommend=1, \n",
    "#              env_seed=0,\n",
    "#              normalize_reward=True,\n",
    "# #              filename=\"/home/anton/Datasets/MovieLens/ml-100k/u.data\",\n",
    "# #              sep='\\t',\n",
    "#              filename=\"/home/anton/Datasets/MovieLens/ml-1m/ratings.dat\",\\\n",
    "#              sep='::',\\\n",
    "# #              session_time = 20 * 60,\n",
    "#              session_size = session_size,\n",
    "#              cache_dir=CACHE_DIR,\n",
    "#              shuffle_sessions=False)\n",
    "    \n",
    "#     env = MovieLensDRR(embedding_dimension=40, \n",
    "#              n_items_to_recommend=1, \n",
    "#              env_seed=seed,\n",
    "#              normalize_reward=True,\n",
    "#              filename=\"/home/anton/Datasets/MovieLens/ml-100k/u.data\",\n",
    "#              sep='\\t',\n",
    "#     #              session_time = 20 * 60,\n",
    "#              session_size = session_size,\n",
    "#              cache_dir=CACHE_DIR,\n",
    "#              shuffle_sessions=True\n",
    "#                       )\n",
    "    env = MovieLens100(\n",
    "                 n_items_to_recommend=1,\n",
    "                 env_seed=seed,\n",
    "                 normalize_reward=False,\n",
    "                 session_time=None,\n",
    "                 session_size=session_size,\n",
    "                 shuffle_sessions=True,\n",
    "                 )\n",
    "    \n",
    "    return StatsWrapper(env)\n",
    "\n",
    "envfn = {\n",
    "    'ml100k_s30_seed10' : lambda : make_env(30, 100)\n",
    "}\n",
    "agents = {\n",
    "        'random' : lambda x : prepare_random_agent_and_env(x),\n",
    "        'ddpg'   : lambda x : prepare_ddpg_agent_and_env(x),\n",
    "        'mf_agent' : lambda x : prepare_mf_agent_and_env(x),\n",
    "}\n",
    "\n",
    "n_steps = [\n",
    "           10000, \n",
    "           #4000, \n",
    "           #16000, \n",
    "           #16000\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8661e02763964bcaad2e21e1e6331364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a89b914ace4c8eaced4f3c86eaf6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421a7c3a3e1b4a7aad7f442012b4cb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/anaconda3/envs/datascience/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb4497f9087463fb8620188dc030136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8bdb0aaeabc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#run_experiment(wrapped, agent, t_train=n_step, t_test=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menvid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-58a810495a4e>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(agent, env, train_steps, eval_steps)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#         print(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/0b9d5a44-bb63-4de1-9cfe-0da47b81e88a/TPT/3A/PRIM Vente Privee/WORK_DIR/rec_gym/agents/ddpg.py\u001b[0m in \u001b[0;36mbegin_episode\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "results = defaultdict( defaultdict) \n",
    "for envid, n_step in tqdm.tqdm_notebook(zip(envfn.keys(), n_steps), leave=False, position=0):\n",
    "\n",
    "    base_env = StatsWrapper(envfn[envid]())\n",
    "    base_env.unwrapped.n_rec\n",
    "\n",
    "    for agent_name, prepare in agents.items():\n",
    "        agent, wrapped, env = prepare(base_env)\n",
    "        #run_experiment(wrapped, agent, t_train=n_step, t_test=0)\n",
    "        train_eval(agent, wrapped, train_steps=1000, eval_steps=1000)\n",
    "        \n",
    "        results[envid][agent_name] = env.interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
