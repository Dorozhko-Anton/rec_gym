{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cloudpickle\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self._storage.append(experience)\n",
    "        if len(self._storage) > self._maxsize:\n",
    "            self._storage.pop(0)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_idx = np.random.choice(range(len(self._storage)), size=batch_size)\n",
    "        return [self._storage[i] for i in batch_idx]\n",
    "\n",
    "#     def save(self, checkpoint_dir, iteration_number):\n",
    "#         cloudpickle.dumps()\n",
    "\n",
    "#     def load(self, checkpoint_dir, iteration_number):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "  def call(self, logits):\n",
    "    # sample a random categorical action from given logits\n",
    "    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, num_actions):\n",
    "    super().__init__('mlp_policy')\n",
    "    # no tf.get_variable(), just simple Keras API\n",
    "    self.hidden1 = kl.Dense(128, activation='relu')\n",
    "    self.hidden2 = kl.Dense(128, activation='relu')\n",
    "    self.value = kl.Dense(1, name='value')\n",
    "    # logits are unnormalized log probabilities\n",
    "    self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "    self.dist = ProbabilityDistribution()\n",
    "def call(self, inputs):\n",
    "    # inputs is a numpy array, convert to Tensor\n",
    "    x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "    # separate hidden layers from the same input tensor\n",
    "    hidden_logs = self.hidden1(x)\n",
    "    hidden_vals = self.hidden2(x)\n",
    "    return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "def action_value(self, obs):\n",
    "    # executes call() under the hood\n",
    "    logits, value = self.predict(obs)\n",
    "    action = self.dist.predict(logits)\n",
    "    # simpler option, will be clear below why we don't use it\n",
    "    # action = tf.random.categorical(logits, 1)\n",
    "    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "\n",
    "class Qsa_network(tf.keras.Model):\n",
    "    def __init__(self, n_inputs, n_hidden, activation='relu'):\n",
    "        super(Qsa_network, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.hidden1 = kl.Dense(self.n_hidden, activation=self.activation)\n",
    "        self.hidden2 = kl.Dense(self.n_hidden, activation=self.activation)\n",
    "        self.value = kl.Dense(1, name='value') \n",
    "        \n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action])\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        value = self.value(x)\n",
    "        return value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Qsa_network(2+2, 10)\n",
    "model.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.utils import Agent\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "class DQN_agent(Agent):\n",
    "    def __init__(self, n_rec, q_network, gamma, replay_size, lr, batch_size):\n",
    "        self.n_rec = n_rec\n",
    "        self.q_network = q_network\n",
    "        self.target_q_network = deepcopy(q_network)\n",
    "    \n",
    "        self.lr = lr\n",
    "        self.replay_size = replay_size\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.replay = ReplayBuffer(self.replay_size)\n",
    "    \n",
    "    def _train_one_step(self):\n",
    "        state, chosen_items, reward, next_state, next_items, done = self.replay.sample(self.batch_size)\n",
    "        \n",
    "        scores = [self.q_network(state, item) in chosen_items]\n",
    "        \n",
    "        target_scores = [self.target_q_network(next_state, item) for item in next_items]\n",
    "        target = \n",
    "        \n",
    "        loss = tf.losses.huber_loss(labels=target, predictions=score)\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(lr=self.lr).minimize(loss)\n",
    "        \n",
    "    \n",
    "    def _predict_action(self, observation):\n",
    "        user, items = observation\n",
    "        scores = [ self._dqn(user, item)\n",
    "                   for item in items ]\n",
    "        action = np.argmax(scores)[::-1][:self.n_rec]\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def begin_episode(self, observation):\n",
    "        action = self._predict_action(observation)\n",
    "        return action\n",
    "\n",
    "    def step(self, reward, observation):\n",
    "        self._save_reward(reward)\n",
    "        action = self._predict_action(observation)\n",
    "        return action\n",
    "\n",
    "    def end_episode(self, reward):\n",
    "        self._save_reward(reward)\n",
    "\n",
    "    def bundle_and_checkpoint(self, directory, iteration):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def unbundle(self, directory, iteration, dictionary):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, num_hiddenNodes1):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], num_hiddenNodes1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hiddenNodes1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, env.action_space.n)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state = torch.tensor(state,dtype=torch.float32,device=device).unsqueeze(0)\n",
    "            q_value = self.forward(state).detach()\n",
    "            action  = q_value.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "\n",
    "def compute_td_loss(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    states = torch.tensor(state,dtype=torch.float32,device=device)\n",
    "    next_states = torch.tensor(next_state,dtype=torch.float32,device=device)\n",
    "    actions = torch.tensor(action,dtype=torch.long, device=device)\n",
    "    rewards = torch.tensor(reward,dtype=torch.float32,device=device)  \n",
    "    dones = torch.tensor(done,dtype=torch.float32,device=device)\n",
    "    \n",
    "    q_values      = model(states)\n",
    "    next_q_values = target_model(next_states).detach()\n",
    "    \n",
    "    q_value          = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = rewards + gamma * next_q_value * (1 - dones)\n",
    "    \n",
    "    #loss = (q_value - expected_q_value).pow(2).mean().unsqueeze(0)\n",
    "    loss = F.smooth_l1_loss(q_value, expected_q_value) # Huber loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rec_gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-15 10:51:16,485] Making new env: JDSimulatedEnv-v2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('JDSimulatedEnv-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.embedding_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, items = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0, 'embedding': array([0.18219419, 0.98326257])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18219419, 0.98326257])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0, 'embedding': array([0.6947017 , 0.71929796]), 'use_until': inf}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
